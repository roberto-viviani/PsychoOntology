---
title: "Models comparison"
author: "Roberto"
date: "2024-04-22"
output: word_document
---

We test the extent to which semantic similarity predicts the respondent correlation of anwers across subscales using different large language models.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(smacof)

embmx <- function(frm) {
  mx = matrix(nrow = nrow(frm), ncol = 3072)
  for (count in 1 : nrow(frm)) {
    eb <- frm[count,"embedding"]
    em <- sub("\\[", "c\\(", eb)
    em <- sub("\\]", "\\)", em)
    c <- eval(str2expression(em))
    mx[count,] <- c
  }
  mx
}

# some traits are misspelled in the original dataset, correction here
traits <- c(
    "emotional lability" = "NegativeAffect",
    "anxiousness" = "NegativeAffect",
    "anxiety" = "NegativeAffect",
    "separation insecurity" = "NegativeAffect",
    "withdrawal" = "Detachment",
    "intimacy avoidance" = "Detachment",
    "intimicy avoidance" = "Detachment",
    "anhedonia" = "Detachment",
    "manipulativeness" = "Antagonism",
    "deceitfulness" = "Antagonism",
    "grandiosity" = "Antagonism",
    "irresponsibility" = "Disinhibition",
    "impulsivity" = "Disinhibition",
    "distractibility" = "Disinhibition",
    "eccentricity" = "Psychoticism",
    "eccentrictiy" = "Psychoticism",
    "perceptual dysregulation" = "Psychoticism",
    "perceptual dysreg" = "Psychoticism",
    "unusual beliefs" = "Psychoticism",
    "perfectionism" = "Anancasticity",
    "rigid perfectionism" = "Anancasticity",
    "rigidity" = "Anancasticity",
    "orderliness" = "Anancasticity"
)
 
scales <- read.csv("scales_data.csv") |> tidyr::drop_na()
getcordata <- function(cdist) {
  cormx <- cor(as.matrix(scales |> select(starts_with("NEOFFI"))), 
      as.matrix(scales |> select(starts_with("PID5BF"))))
  cordata <- data.frame(corrs = as.vector(cormx), cosdist = as.vector(cdist),
                        typeNEO = NEO$type, typePID = rep(PID$type, each = nrow(NEO)),
                        itemNEO = NEO$itemID, itemPID = rep(PID$itemID, each = nrow(NEO)))
  cordata$traitPID <- traits[cordata$typePID]
  cordata
}

# plot function
plotfunc <- function(data) {
  ggplot(
    data |> group_by(typeNEO, traitPID) |> summarize(corr = mean(corrs), dist = mean(cosdist)),
    aes(y = abs(corr), x = dist)) + 
    geom_point(aes(color = typeNEO, shape = traitPID), alpha = 0.8, size = 4) +
    geom_smooth(method = "glm", method.args = c("family"="quasibinomial"), 
      alpha = 0.2, color = "darkgrey") + 
    labs(color = "NEO scale", shape = "PID trait") + 
    xlab("semantic distance") + ylab("avg correlation") + theme_classic()
}

# model and inference
testfunc <- function(data) {
  summary(glm(abs(corr) ~ dist, 
    data = data |> group_by(typeNEO, traitPID) |> summarize(corr = mean(corrs), dist = mean(cosdist)),             family = quasibinomial))
}

```

## OpenAI, small vector embeddings

```{r, message = FALSE, warning = FALSE}
embeddings <- read.csv("embeddings_openAI_small_de.csv")
NEO <- filter(embeddings, scaleID == "NEO") |> select(itemID, type, embedding)
PID <- filter(embeddings, scaleID == "PID") |> select(itemID, type, embedding)
cordata  <- getcordata(crossdist(embmx(NEO), embmx(PID)))

plotfunc(cordata)
testfunc(cordata)
rm(list = c("embeddings", "NEO", "PID", "cordata"))
```

## Mistral

```{r, message = FALSE, warning = FALSE}
embeddings <- read.csv("embeddings_mistral_de.csv")
NEO <- filter(embeddings, scaleID == "NEO") |> select(itemID, type, embedding)
PID <- filter(embeddings, scaleID == "PID") |> select(itemID, type, embedding)
cordata  <- getcordata(crossdist(embmx(NEO), embmx(PID)))

plotfunc(cordata)
testfunc(cordata)
rm(list = c("embeddings", "NEO", "PID", "cordata"))
```

## Roberta

```{r, message = FALSE, warning = FALSE}
embeddings <- read.csv("embeddings_roberta_de.csv")
NEO <- filter(embeddings, scaleID == "NEO") |> select(itemID, type, embedding)
PID <- filter(embeddings, scaleID == "PID") |> select(itemID, type, embedding)
cordata  <- getcordata(crossdist(embmx(NEO), embmx(PID)))

plotfunc(cordata)
testfunc(cordata)
rm(list = c("embeddings", "NEO", "PID", "cordata"))
```

## all-mpnet-base-v2

```{r, message = FALSE, warning = FALSE}
embeddings <- read.csv("embeddings_mpnetbasev2_de.csv")
NEO <- filter(embeddings, scaleID == "NEO") |> select(itemID, type, embedding)
PID <- filter(embeddings, scaleID == "PID") |> select(itemID, type, embedding)
cordata  <- getcordata(crossdist(embmx(NEO), embmx(PID)))

plotfunc(cordata)
testfunc(cordata)
rm(list = c("embeddings", "NEO", "PID", "cordata"))
```

### Comparison models

The comparison reveals that openAI is the best model, followed by roberta and finally by mistral.

## Averaged models

We look at whether performance can be improved by combining the information from models. We do not use mpnet because it performs poorly, but include mistral as it may provide different information.

```{r, message = FALSE, warning = FALSE}
embeddings <- read.csv("embeddings_openAI_small_de.csv")
NEO <- filter(embeddings, scaleID == "NEO") |> select(itemID, type, embedding)
PID <- filter(embeddings, scaleID == "PID") |> select(itemID, type, embedding)
distmx_openAI <- crossdist(embmx(NEO), embmx(PID))

embeddings <- read.csv("embeddings_mistral_de.csv")
NEO <- filter(embeddings, scaleID == "NEO") |> select(itemID, type, embedding)
PID <- filter(embeddings, scaleID == "PID") |> select(itemID, type, embedding)
distmx_mistral <- crossdist(embmx(NEO), embmx(PID))

embeddings <- read.csv("embeddings_roberta_de.csv")
NEO <- filter(embeddings, scaleID == "NEO") |> select(itemID, type, embedding)
PID <- filter(embeddings, scaleID == "PID") |> select(itemID, type, embedding)
distmx_roberta <- crossdist(embmx(NEO), embmx(PID))

prtr1 <- Procrustes(distmx_openAI, distmx_mistral)
prtr2 <- Procrustes(distmx_openAI, distmx_roberta)
distmx_pooled <- (distmx + prtr1$Yhat + prtr2$Yhat) / 3

cordata_pooled <- getcordata(distmx_pooled)
plotfunc(cordata_pooled)
testfunc(cordata_pooled)
rm(list = c("embeddings", "NEO", "PID", "distmx_openAI", "distmx_mistral", 
     "distmx_roberta", "prtr1", "prtr2", "cordata_pooled"))
```

This reveals that the average model performs between the best and the worst model. There appears no advantage in pooling the infromation acrss models; the best model is better than other combinations.