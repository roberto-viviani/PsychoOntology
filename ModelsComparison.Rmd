---
title: "Models comparison"
author: "Roberto"
date: "2024-04-22"
output: word_document
---

We test the extent to which semantic similarity predicts the respondent correlation of answers across subscales using different large language models.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(smacof)

# utilities to decode embeddings
source("embedding_utilities.R") 

scales <- read.csv("scales_data.csv") |> tidyr::drop_na()
getcordata <- function(cdist) {
  cormx <- cor(as.matrix(scales |> select(starts_with("NEOFFI"))), 
      as.matrix(scales |> select(starts_with("PID5BF"))))
  cordata <- data.frame(corrs = as.vector(cormx), cosdist = as.vector(cdist),
                        typeNEO = NEO$type, typePID = rep(PID$type, each = nrow(NEO)),
                        itemNEO = NEO$itemID, itemPID = rep(PID$itemID, each = nrow(NEO)))
  cordata$traitPID <- traits[cordata$typePID]
  cordata
}

# plot function
plotfunc <- function(data) {
  ggplot(
    data |> group_by(typeNEO, traitPID) |> summarize(corr = mean(corrs), dist = mean(cosdist)),
    aes(y = abs(corr), x = dist)) + 
    geom_point(aes(color = typeNEO, shape = traitPID), alpha = 0.8, size = 4) +
    geom_smooth(method = "glm", method.args = c("family"="quasibinomial"), 
      alpha = 0.2, color = "darkgrey") + 
    labs(color = "NEO scale", shape = "PID trait") + 
    xlab("semantic distance") + ylab("avg correlation") + theme_classic()
}

# model and inference
testfunc <- function(data) {
  summary(glm(abs(corr) ~ dist, 
    data = data |> group_by(typeNEO, traitPID) |> summarize(corr = mean(corrs), dist = mean(cosdist)),             family = quasibinomial))
}

```

## OpenAI, small vector embeddings

```{r, message = FALSE, warning = FALSE}
embeddings <- read.csv("embeddings_openAI_small_de.csv")
NEO <- filter(embeddings, scaleID == "NEO") |> select(itemID, type, embedding)
PID <- filter(embeddings, scaleID == "PID") |> select(itemID, type, embedding)
cordata  <- getcordata(crcosdist(embmx(NEO), embmx(PID)))

plotfunc(cordata)
#ggsave("subscales_openAI_small.pdf", width = 150, height = 95, units = "mm")
testfunc(cordata)
rm(list = c("embeddings", "NEO", "PID", "cordata"))
```

## Mistral

```{r, message = FALSE, warning = FALSE}
embeddings <- read.csv("embeddings_mistral_de.csv")
NEO <- filter(embeddings, scaleID == "NEO") |> select(itemID, type, embedding)
PID <- filter(embeddings, scaleID == "PID") |> select(itemID, type, embedding)
cordata  <- getcordata(crcosdist(embmx(NEO), embmx(PID)))

plotfunc(cordata)
#ggsave("subscales_mistral.pdf", width = 150, height = 95, units = "mm")
testfunc(cordata)
rm(list = c("embeddings", "NEO", "PID", "cordata"))
```

## Roberta

```{r, message = FALSE, warning = FALSE}
embeddings <- read.csv("embeddings_roberta_de.csv")
NEO <- filter(embeddings, scaleID == "NEO") |> select(itemID, type, embedding)
PID <- filter(embeddings, scaleID == "PID") |> select(itemID, type, embedding)
cordata  <- getcordata(crcosdist(embmx(NEO), embmx(PID)))

plotfunc(cordata)
#ggsave("subscales_roberta_small.pdf", width = 150, height = 95, units = "mm")
testfunc(cordata)
rm(list = c("embeddings", "NEO", "PID", "cordata"))
```

## all-mpnet-base-v2

Like Roberta, this is a freely available model.

```{r, message = FALSE, warning = FALSE}
embeddings <- read.csv("embeddings_mpnetbasev2_de.csv")
NEO <- filter(embeddings, scaleID == "NEO") |> select(itemID, type, embedding)
PID <- filter(embeddings, scaleID == "PID") |> select(itemID, type, embedding)
cordata  <- getcordata(crcosdist(embmx(NEO), embmx(PID)))

plotfunc(cordata)
testfunc(cordata)
rm(list = c("embeddings", "NEO", "PID", "cordata"))
```

## Universal Sentence Encoder

Universal Sentence Encoder embeddings are not based on a language model and are publicly available on the TernsorFlow site.

```{r, message = FALSE, warning = FALSE}
embeddings <- read.csv("embeddings_univSentEncoder_de.csv")
NEO <- filter(embeddings, scaleID == "NEO") |> select(itemID, type, embedding)
PID <- filter(embeddings, scaleID == "PID") |> select(itemID, type, embedding)
cordata  <- getcordata(crcosdist(embmx(NEO), embmx(PID)))

plotfunc(cordata)
#ggsave("subscales_univSentEnc.pdf", width = 150, height = 95, units = "mm")
testfunc(cordata)
rm(list = c("embeddings", "NEO", "PID", "cordata"))
```

The performance of this modest is the lowest among all considered models.

### Comparison models

The comparison reveals that openAI is the best model, followed by roberta and finally by mistral.

## Averaged models

We look at whether performance can be improved by combining the information from models. We do not use mpnet because it performs poorly, but include mistral as it may provide different information.

```{r, message = FALSE, warning = FALSE}
embeddings <- read.csv("embeddings_openAI_small_de.csv")
NEO <- filter(embeddings, scaleID == "NEO") |> select(itemID, type, embedding)
PID <- filter(embeddings, scaleID == "PID") |> select(itemID, type, embedding)
distmx_openAI <- crcosdist(embmx(NEO), embmx(PID))

embeddings <- read.csv("embeddings_mistral_de.csv")
NEO <- filter(embeddings, scaleID == "NEO") |> select(itemID, type, embedding)
PID <- filter(embeddings, scaleID == "PID") |> select(itemID, type, embedding)
distmx_mistral <- crcosdist(embmx(NEO), embmx(PID))

embeddings <- read.csv("embeddings_roberta_de.csv")
NEO <- filter(embeddings, scaleID == "NEO") |> select(itemID, type, embedding)
PID <- filter(embeddings, scaleID == "PID") |> select(itemID, type, embedding)
distmx_roberta <- crcosdist(embmx(NEO), embmx(PID))

prtr1 <- Procrustes(distmx_openAI, distmx_mistral)
prtr2 <- Procrustes(distmx_openAI, distmx_roberta)
distmx_pooled <- (distmx_openAI + prtr1$Yhat + prtr2$Yhat) / 3

cordata_pooled <- getcordata(distmx_pooled)
plotfunc(cordata_pooled)
testfunc(cordata_pooled)
rm(list = c("embeddings", "NEO", "PID", "distmx_openAI", "distmx_mistral", 
     "distmx_roberta", "prtr1", "prtr2", "cordata_pooled"))
```

This reveals that the average model performs between the best and the worst model. There appears no advantage in pooling the infromation acrss models; the best model is better than other combinations.